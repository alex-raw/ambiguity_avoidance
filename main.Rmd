---
author: |
  | Alexander Rauhut
  | Institut für Englische Philologie
  | Freie Universität Berlin
title: "Inflection across the verb-noun continuum: The role of potential lexical ambiguity caused by conversion"
date: 2021-07-15

output:
    pdf_document:
        keep_tex: true
        toc: true
        number_sections: true
        fig_caption: true

citation_package: biblatex
bibliography: uni.bib
csl: styles/unified-style-sheet-for-linguistics.csl
link-citations: yes

header-includes:
  - \usepackage{caption}
  - \usepackage{subcaption}

---

\newpage

``` {R imports, echo=FALSE, include = FALSE}
library(parameters)
coef_tables <- readRDS("R/coef_tables")
resid_table <- readRDS("R/resid_table")
summaries <- readRDS("R/summaries")
Rsq_vals <- readRDS("R/Rsq_vals")
resid_table <- rbind(resid_table, c(list(coefficient = "pseudo R²"), Rsq_vals))
resid_table[-1] <- lapply(resid_table[-1], as.numeric)
```

# Introduction

Ambiguity is pervasive in language. blablabla

Conversion is a productive process that inevitably produces homophones.
Additionally, noun-verb and verb-noun conversions are doubly homophonous with inflectional affixes in *-s*.

This study will investigate potential large scale effects that the homonymy of *-s* has.

The emergence of lexical clusters is conditioned by frequency.
Various studies have provided evidence [review in @diessel16]

In addition to the shared form, there is extensive conversion in the English language, allowing verbs to be readily used as nouns and vice-versa.
However, the function of verbal *-s* and nominal *-s* is quite different, and the immediate grammatical context can be expected to be enough for disambiguation.

In section 2, I will review some of the literature on ambiguity and ambiguity avoidance and gather information from other fields of linguistics.
Section 3 is an explorative overview over some ambiguous uses of *-s*.
The approach taken is using POS ambiguity tags and ranked association values (log-likelihood ratio/LLR) in the same spirit as collostruction analysis [@stefanowitsch+gries03].
In addition, I correlated the resulting association values with cosine similarities between the base forms and affixed forms
taken from a simple GloVe embedding [@pennington14; @text2vec] trained on the respective datasets.
The result shows that larger differences between base and affixed form lead to significantly fewer ambiguous tags.

Finally, in section 4, I will present a series of GAMLSS models (Generalized Additive Models for Location, Scale and Shape [@stasinopoulos18; @gamlss])
that attempt to model the influence of verb-noun or noun-verb conversion on the distribution of *-s* and *-ed* suffixation
while controlling for textual dispersion, regularity of occurrence and contextual flexibility. [^1]

[^1]: The data was preprocessed using the *Corpus Workbench* [@cwb], all analyses were coded in *R* 4.1.0 [@R_base; @data.table], figures were created with *ggplot2* [@ggplot].

# Avoiding Ambiguity

formal equivalence
homonymy and polysemy

that deletion is not permitted in non-extraposed subject clauses
@wasow15

This paper demonstrates that the differences in duration of English final S as a function of the morphological function it expresses.

Hypothesis: third person singular -s is avoided in noun-verb conversion

In fact, it is difficult to come up with genuine examples of structural ambiguity caused by the homonymy of *-s*.
<!-- true? look for ambiguous examples in corpus -->

(@) The eye of the artist is concentrated on his pencil ; the pencil <moves> and the line dreams. (BNC: 49864)


Studies like @plag17 call into question whether there is perfect homonymy between different types of word final *-s*. The study found significant length differences, and hypothesizes that
@song_et_al13 did not find any such effect, however, albeit on a more lexically restricted study.

In a more recent study, @tomaschek_et_al21 show that length of final *-s* can be modelled as having a discriminatory function depending on the lexical and phonological context.
In other words, the duration was found to decrease with increasing contextual ambiguity. <!-- TODO: this doesn't sound right-->
"Energy is not invested in a signal that creates confusion instead of clarity." [@tomaschek_et_al21, p. 154]
This points to some degree of ambiguity sensitivity, even though it is not entirely clear what it means for ambiguity avoidance.

Systematic phonetic differences potentially contribute to the disambiguation

@wasow15 surveys a variety of studies ...
and concludes that ambiguity avoidance has not been shown to be as common as
expected considering the pervasiveness of ambiguities in language.
Grice underestimated the pervasiveness of ambiguity.
Provides an attempt at a taximony.
- word order freezing as ambiguity avoidance
- that deletion only in unambiguous syntactic context
- German free syntax but default SVO reading.
    Wasow does not call it default.

@piantadosi_et_al12
Some degree of ambiguity is required in an efficient communicative system.
ease of processing blabla

@trott+bergen20:
Linguistic forms are expected to show a certain degree of ambiguity in order to provide an efficient system.
Simulations suggest that ambiguity caused by homophony is more common in natural languages than expected, even when taking into account.
<!-- TODO: figure out how expected homophone ratios are calculated -->
Homophones are smoothed out in lexically neighboring areas.

## Conversion

Conversion is fairly common in the English language.

# Methodology
## Data

Two datasets were used.
As a general corpus, the traditional BNC [@bnc],
and as a more specific text type, the spoken BNC2014 [@bnc2014] was chosen for comparison.
The basic unit of analysis was lemmatized tokens, in combination with the CLAWS pos tags.
It should be noted that lemmatization and pos tagging are not equivalent across the two datasets,
but the results should be similar enough to have little overall effect on the following analyses.
Larger samples might be desirable.

The dataset was finally broken up into verbs and nouns based on what they occur as more often.
In the rare case of a tie, the item was included in both data sets.
All metrics, however, were calculated on all occurrences of a lemma rather than only occurrences tagged as noun or verb as it is typically done.
Like that, it is possible to capture a wider range of statistics per item,
while also minimizing any a priori categorization of items,
acknowledging the fact that lexical items in English are rather flexible when it comes to open word classes.

Textual co-occurrence can only serve as an upper bound.
Prosodic markers in spoken language can further disambiguate most types of ambiguity.
The same is true for more general types of background knowledge.
In general, it can be expected that there tends to be enough structural and contextual information to disambiguate *-s*.

## Measuring conversion

from -1 for verbal uses to +1 for nominal uses and 0 for 50/50
show LLR graph to show that it is continuous and monotonic

Proper names were commonly mistagged as verbs in the datasets, causing heavy tails in the residuals, therefore excluded from the final analysis.

## Textual specificity

In order to measure the textual specificity of lexemes,
and penalize forms with very specific contextually bound uses,
two types of dispersion are used.
The first is the Deviation of Proportions across corpus parts (DP.norm) [@gries08], more specifically the normalized version [cf. @lijffijt+gries12].
As the basic of unit, the individual texts were used by text ID.
DP.norm is a corpus-part-based measure bound between 0 and 1.
Values close to 1 indicate a high deviation, therefore a low dispersion.
It can be interpreted as a measure of how evenly tokens are spread over the corpus parts.

DP cannot account for short bursts of occurrences,
therefore, Word Growth Dispersion (DWG) [@zimmermann20] is used in addition,
which is a whole-corpus measure based on distances between occurrences.
DWG is a measure of how regularly a token occurs across an entire corpus occurs across an entire corpus.

If ambiguity is regularly avoided, it should be expected that with growing word class ambiguity, the use of *-s* becomes more context-dependent.
Considering a given dispersion or "clumpiness" of contexts in which a lexeme is likely to occur,
the existence of avoidance-contexts should cause a penalty to dispersion and make the distribution clumpier.

Both measures were designed to measure dispersion or commonness of lexical items as extension to the most commonly used plain frequencies,
The two measures highlight different aspects both conceptually and empirically [cf. @gries21 on using multiple measures of dispersion and association].

Problem:
The contexts in which ambiguity is avoided/not avoided might be rather evenly dispersed themselves.
This could mask potential clumpiness of *-s* occurrences.

It is difficult to identify avoidance contexts formally.
Lexical and syntactical correlates of the avoided structure might be avoided as a side effect, and cause fuzzier overall differences in structure.

## Lexical specificity

The ratio of hapaxes [henceforth $\alpha\textsubscript{1}$ @evert05: 130] on either side of a given lexeme was used as a simple measure of how fixed the immediate lexico-grammatical context is.
The term *hapax* is used a bit more liberally here and refers to types that occur only once in the given window.
For ease of interpretation and in order to capture the fixedness in smaller units of text, the window was held at 1 token to the left and right.
Preliminary experiments with larger windows were inconclusive, so the simplest version was used.
The left and right contexts were also kept separate
since lumping them together conflates quite different pieces of information,
considering the direction of processing, the branching structure of English, etc.
For example, a token that is always preceded by a definite article as a part of a name, scores an extremely low $\alpha\textsubscript{1}$ value.

Lemmas that had *-s* forms that only occurred in totally fixed contexts, therefore scoring alpha (Hapax) values of lower than 0.3 were excluded as outliers.
Exclusion of those observations improved distributional properties of the models.
Examples for those outliers are abbreviations (e.g. *e.g.*, *pp.*, *etc.*, etc.), and parts of multi-word names.

On the lower frequency bands this is due to a substantial decrease of noise as it is difficult to estimate the proportion of *-s* only based on a few occurrences of *-s*.
For the higher frequency bands, it is in many cases equal to the exclusion of mostly names and idiomatic expressions.
<!-- TODO: include example -->
This is conceptually unproblematic since multi-word structures should be treated as individual units, and as a different category of lexeme.

# Analysis
## Overview

## Modelling inflection ratio

While controlling for clumpiness with dwg did not substantially change the effect
of conversion, it did improve the fit of the model.

why gam: [@gam; @wood17]
Semi-parametric
Both extremely high and extremely low values of dwg suggest special values.
Most of the variables used in the model are not expected to have a monotonic relationship with the proportion of *-s* occurrences.
Very badly dispersed lexemes are overrepresented in terms of frequency,
and an extremely even dispersion suggests uses typical of function words.
The same is true for productivity or contextual fixedness.
Both extremely high and extremely low values are untypical.

Predicting counts of *-s* directly with poisson or negative binomial regression leads to problematic model properties such as highly skewed residuals.
Human perception has been found to be more sensitive to proportional changes of stimuli rather than absolute ones [cf. @kromer03].
With that in mind, the independent variable to be predicted was set to the proportion of occurrences of *-s*.
As such the possible values can range from 0 to 1 inclusively.
Therefore, the distribution chosen to be fitted was a zero-one-inflated beta distribution [@ospina+ferrari12; @rigby19].
To allow for the necessary flexibility in the distribution, the model framework chosen was GAMLSS that allow the use of smoothing spline and fitting the required inflated beta distribution. The models were created with the R package *gamlss* [@gamlss].

## Models

### Model fit

Table \ref{fit} shows the statistics for the residual distribution

``` {R, fit, echo = FALSE}
setNames(resid_table, c("Metric", "verbal -s", "nominal -s", "ed")) |>
knitr::kable(digits = 4,
    caption = "\\label{fit}Statistics for residuals and fit of the three GAMLSS models")
```

Means and variances are very close to the desired values.
The model for verbal *-s* also shows a heavy right tail,
which is caused by a high amount of extreme values.
Arguably, this is not unusual for language data, and might be related to the sample size.
All models show some degree of skew.
An inherent property of the dataset is a potential for multi-modality since the basic unit of analysis is lemmas.
Clustering techniques or fitting model mixes might be strategies for improvement,
without making arbitrary assumptions on lexical classes and creating cut-off points,
e.g. for dominantly nominal vs. dominantly verbal lexemes.

The overall deviance explained by the models is rather high for the verbal *-s* model and medium for the other two models.
There is much more variation on the nominal side of the spectrum, hence more variation when it comes to plural *-s*, which is also the most frequent inflection in comparison.

## *-s* vs *-ed*

\begin{figure}
     \centering
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/m_verb_mu_conversion.jpg}
         \caption{verbal \textit{-s}}
         \label{verb_mu_conv}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/m_noun_mu_conversion.jpg}
         \caption{nominal \textit{-s}}
         \label{noun_mu_conv}
     \end{subfigure}
        \caption{GAMLSS estimates for conversion}
        \label{conv_pair}
\end{figure}

Figure \ref{conv_pair} shows a side by side comparison of the estimates for conversion.
The shape is similar, but the variability in the case of nominal s is much larger.
In both cases, there is a depression towards the middle of the continuum rather than a simple monotonic decrease. In comparison, the influence of conversion is more linear for *-ed* as can be seen in figure \ref{ed_mu_conv}.

\begin{figure}
     \centering
     \includegraphics[width=0.5\textwidth]{figures/m_ed_mu_conversion.jpg}
     \caption{GAMLSS estimate for conversion for \textit{-ed}}
     \label{ed_mu_conv}
\end{figure}

Plain, relative or log frequencies did not improve the models.
In fact, model diagnostics became much worse in some cases.
Possible reasons are the problematic distributional properties of word frequencies and low frequency noise.
Furthermore, frequency influences every of the remaining metrics, so it is in a sense encoded there.
Therefore, it was dropped in the final models presented here.

# Discussion

bla

# Outlook and Conclusion

Modelling the proportion of *-s* occurrences showed a negative effect for items that are more likely to occur in conversion.
This pattern was more pronounced for the nominal side of the continuum.
Additionally, the probability that items never occur with the *-s* suffix of their respective dominant word class was shown to drastically decrease already at rather low proportions of conversion.
However, the predictions of the model have to be taken with a grain of salt, since the distributional properties of the data could not be fitted perfectly, resulting in skewed residuals.
Some potential factors could be the non-randomness of the data, strong systematic noise, such as homography, and potential unaccounted multimodality, e.g. caused by other word classes.
Nevertheless, the applied dispersion and specificity measures allowed to improve the fit drastically, and show promise for future improvements.
Additionally, understanding morphological data as proportional, rather than count data, allows for intuitive and conceptually interesting interpretations.

*-ed* showed a monotonically increasing effect towards the nominal side of the spectrum indicating that nouns, if they are used as verbs, are slightly more likely to be marked with the *-ed* suffix.
This effect is too slight to be conclusive, but it in comparison to *-s* there is no significant dip in the middle of the continuum.
This should be expected if ambiguity plays a role.

Word embeddings and transformer networks like BERT may provide an interesting route for further studies in ambiguity [e.g. @beekhuizen_et_al21; @du_et_al19; @wiedemann19].
Clustering techniques can be used to detect homonymy [@lee21], which could be used in corpus annotation as a replacement of classical lemmatization.
If robust and carefully applied, this could potentially lead to decreases in noise.
Those recent successes in practical application are promising for the use in more descriptive application.
They can provide another angle on co-occurrence statistics, and were only sparsely used in this study since there has been little systematic application in corpus linguistics.
Include more contextual and "world knowledge" information, such as images [e.g. @kottur16; @shahmohammadi21].
Recent advances in context embedding might provide tools to further test where real ambiguity exists and where it affects the system of language.

More sophisticated measures of lexical fixedness/productivity than the proposed fixed-window hapax ratio are also desirable,
especially to capture constructions, constructional idioms and other semi-fixed structures.

There are no significant differences in spread and frequency of *-s* in lexical forms that occur as verb-noun or noun-verb conversions. (???)

\newpage

# Appendix{-}

## Model summaries

### Verbal *-s*

\small

``` {R, coefs_verb, echo = FALSE, comment = NA}
print(coef_tables$m_verb)
summaries$m_verb |> paste("\n") |> cat()
```

\normalsize
\newpage

### Nominal *-s*

\small
``` {R, coefs_noun, echo = FALSE, comment = NA, fig.cap = "test"}
print(coef_tables$m_noun)
summaries$m_noun |> paste("\n") |> cat()
```

\normalsize
\newpage

### *-ed*

\small

``` {R, coefs_ed, echo = FALSE, comment = NA}
print(coef_tables$m_ed)
summaries$m_ed |> paste("\n") |> cat()
```

\normalsize
\newpage

## Diagnostics {#diagn}

\begin{figure}
     \centering
     \includegraphics[width=\textwidth]{figures/m_verb_diagnostic.jpg}
     \caption{Diagnostics for beta inflated GAMLSS model predicting proportions of verbal \textit{-s}}
     \label{verb_diagnostics}
\end{figure}

\begin{figure}
     \centering
     \includegraphics[width=\textwidth]{figures/m_noun_diagnostic.jpg}
     \caption{Diagnostics for beta inflated GAMLSS model predicting proportions of nominal \textit{-s}}
     \label{noun_diagnostics}
\end{figure}

\begin{figure}
     \centering
     \includegraphics[width=\textwidth]{figures/m_ed_diagnostic.jpg}
     \caption{Diagnostics for beta inflated GAMLSS model predicting proportions of \textit{-ed}}
     \label{ed_diagnostics}
\end{figure}

\newpage

# Bibliography{-}

<!-- vim: set ft=markdown: -->
